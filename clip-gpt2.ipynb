{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe46ce3",
   "metadata": {},
   "source": [
    "# CLIP + GPT-2 Image Captioning Model\n",
    "\n",
    "This notebook implements an image captioning system that combines CLIP's vision encoder with GPT-2's language generation capabilities. The model learns to generate descriptive captions for images using the Flickr30k dataset.\n",
    "\n",
    "## 🔧 Setup and Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db37adb-7a64-48a3-bf84-a29c75fac6d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:44:34.350515Z",
     "iopub.status.busy": "2025-09-10T13:44:34.350246Z",
     "iopub.status.idle": "2025-09-10T13:46:39.231506Z",
     "shell.execute_reply": "2025-09-10T13:46:39.230729Z",
     "shell.execute_reply.started": "2025-09-10T13:44:34.350496Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-10 13:46:21.400969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757511981.788420      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757511981.898376      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install transformers torch torchvision datasets accelerate wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    CLIPVisionModel, CLIPProcessor,\n",
    "    GPT2LMHeadModel, GPT2Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f05b3b",
   "metadata": {},
   "source": [
    "## 📊 Dataset Preparation\n",
    "\n",
    "Setting up the Flickr30k dataset loader with robust CSV parsing to handle different file formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c24447-4a91-4be8-9e01-0c8aea5d1bf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:46:39.235888Z",
     "iopub.status.busy": "2025-09-10T13:46:39.235601Z",
     "iopub.status.idle": "2025-09-10T13:46:39.257218Z",
     "shell.execute_reply": "2025-09-10T13:46:39.256457Z",
     "shell.execute_reply.started": "2025-09-10T13:46:39.235858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, clip_processor, gpt2_tokenizer, max_length=50):\n",
    "        self.images_dir = images_dir\n",
    "        self.clip_processor = clip_processor\n",
    "        self.gpt2_tokenizer = gpt2_tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load CSV with correct separator\n",
    "        print(f\"Loading dataset from: {csv_file}\")\n",
    "        self.data = self.load_csv_robust(csv_file)\n",
    "        \n",
    "        print(f\"Dataset shape: {self.data.shape}\")\n",
    "        print(f\"Columns: {list(self.data.columns)}\")\n",
    "        self.prepare_data()\n",
    "        self.gpt2_tokenizer.pad_token = self.gpt2_tokenizer.eos_token\n",
    "        try:\n",
    "            image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            print(f\"Found {len(image_files)} images in {images_dir}\")\n",
    "        except:\n",
    "            print(f\"Warning: Could not access images directory {images_dir}\")\n",
    "        \n",
    "    def load_csv_robust(self, csv_file):\n",
    "        strategies = [\n",
    "            {'sep': '|'},\n",
    "            \n",
    "            {'sep': '|', 'on_bad_lines': 'skip'},\n",
    "            \n",
    "            {'sep': ',', 'on_bad_lines': 'skip'},\n",
    "            \n",
    "            {'sep': '\\t', 'on_bad_lines': 'skip'},\n",
    "        ]\n",
    "        \n",
    "        for i, params in enumerate(strategies):\n",
    "            try:\n",
    "                print(f\"Trying strategy {i+1}: {params}\")\n",
    "                df = pd.read_csv(csv_file, **params)\n",
    "                if len(df.columns) > 1:\n",
    "                    print(f\"✅ Successfully loaded with strategy {i+1}\")\n",
    "                    print(f\"Shape: {df.shape}, Columns: {list(df.columns)}\")\n",
    "                    return df\n",
    "                else:\n",
    "                    print(f\"⚠️ Strategy {i+1} only found 1 column, trying next...\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Strategy {i+1} failed: {str(e)[:100]}\")\n",
    "                continue\n",
    "        raise ValueError(\"All CSV loading strategies failed!\")\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare data based on the actual CSV structure\"\"\"\n",
    "        print(\"Original columns:\", list(self.data.columns))\n",
    "        if len(self.data.columns) >= 3:\n",
    "            # Standard format: image_name | comment_number | comment\n",
    "            self.data['image'] = self.data.iloc[:, 0]  # First column: image name\n",
    "            self.data['caption'] = self.data.iloc[:, 2]  # Third column: caption\n",
    "            print(\"Using columns 0 (image) and 2 (caption)\")\n",
    "        elif len(self.data.columns) >= 2:\n",
    "            # Two columns: image | caption\n",
    "            self.data['image'] = self.data.iloc[:, 0]\n",
    "            self.data['caption'] = self.data.iloc[:, 1]\n",
    "            print(\"Using columns 0 (image) and 1 (caption)\")\n",
    "        else:\n",
    "            raise ValueError(\"CSV must have at least 2 columns\")\n",
    "\n",
    "        initial_size = len(self.data)\n",
    "        self.data = self.data.dropna(subset=['caption'])\n",
    "        self.data = self.data[self.data['caption'].astype(str).str.len() > 0]\n",
    "        final_size = len(self.data)\n",
    "        \n",
    "        print(f\"Removed {initial_size - final_size} rows with missing/empty captions\")\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Final dataset prepared with {len(self.data)} samples\")\n",
    "        if len(self.data) > 0:\n",
    "            sample = self.data.iloc[0]  # ✅ Fixed: added [0] index\n",
    "            print(f\"Sample - Image: {sample.get('image', 'N/A')}\")\n",
    "            print(f\"Sample - Caption: {str(sample.get('caption', 'N/A'))[:100]}...\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        image_name = str(row['image']).strip()\n",
    "        if not image_name.lower().endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
    "            image_name += '.jpg'\n",
    "            \n",
    "        image_path = os.path.join(self.images_dir, image_name)\n",
    "        caption = str(row['caption']).strip()\n",
    "        \n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
    "            pixel_values = image_inputs['pixel_values'].squeeze(0)\n",
    "        except Exception as e:\n",
    "            # Create dummy image for missing files\n",
    "            pixel_values = torch.zeros(3, 224, 224)\n",
    "        \n",
    "        # Process caption\n",
    "        caption_tokens = self.gpt2_tokenizer.encode(\n",
    "            caption, \n",
    "            add_special_tokens=True, \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'caption_tokens': caption_tokens,\n",
    "            'caption_text': caption\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f43649",
   "metadata": {},
   "source": [
    "## 🏗️ Model Architecture\n",
    "\n",
    "Building the CLIP-GPT2 model that bridges vision and language understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3d21a14-1418-4ff0-9b4b-390b028de137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T16:34:18.962013Z",
     "iopub.status.busy": "2025-09-10T16:34:18.961284Z",
     "iopub.status.idle": "2025-09-10T16:34:18.980727Z",
     "shell.execute_reply": "2025-09-10T16:34:18.980036Z",
     "shell.execute_reply.started": "2025-09-10T16:34:18.961972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ClipGPT2Model(nn.Module):\n",
    "    def __init__(self, clip_model_name=\"openai/clip-vit-base-patch32\", \n",
    "                 gpt2_model_name=\"gpt2\", mapping_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.clip_vision = CLIPVisionModel.from_pretrained(clip_model_name)\n",
    "        self.clip_vision.eval()  # Freeze CLIP during training\n",
    "        for param in self.clip_vision.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
    "        \n",
    "\n",
    "        if self.gpt2.config.pad_token_id is None:\n",
    "            self.gpt2.config.pad_token_id = self.gpt2.config.eos_token_id\n",
    "\n",
    "        clip_dim = self.clip_vision.config.hidden_size  # 512 for base CLIP\n",
    "        gpt2_dim = self.gpt2.config.hidden_size  # 768 for GPT-2\n",
    "        \n",
    "        self.clip_to_gpt2 = nn.Sequential(\n",
    "            nn.Linear(clip_dim, mapping_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(mapping_dim, gpt2_dim),\n",
    "            nn.LayerNorm(gpt2_dim)\n",
    "        )\n",
    "        self.prefix_length = 10\n",
    "        \n",
    "    def get_image_features(self, pixel_values):\n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_vision(pixel_values=pixel_values).pooler_output\n",
    "        return image_features\n",
    "    \n",
    "    def forward(self, pixel_values, caption_tokens):\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        \n",
    "        image_features = self.get_image_features(pixel_values)  # [batch_size, 512]\n",
    "        \n",
    "        image_embeddings = self.clip_to_gpt2(image_features)  # [batch_size, 768]\n",
    "\n",
    "        image_embeddings = image_embeddings.unsqueeze(1).expand(\n",
    "            batch_size, self.prefix_length, -1\n",
    "        )\n",
    "        \n",
    "        caption_embeddings = self.gpt2.transformer.wte(caption_tokens)  # [batch_size, seq_len, 768]\n",
    "        \n",
    "        \n",
    "        combined_embeddings = torch.cat([image_embeddings, caption_embeddings], dim=1)\n",
    "        \n",
    "        \n",
    "        image_mask = torch.ones(batch_size, self.prefix_length, device=pixel_values.device)\n",
    "        caption_mask = (caption_tokens != self.gpt2.config.pad_token_id).float()\n",
    "        attention_mask = torch.cat([image_mask, caption_mask], dim=1)\n",
    "        \n",
    "        \n",
    "        outputs = self.gpt2(\n",
    "            inputs_embeds=combined_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=torch.cat([\n",
    "                torch.full((batch_size, self.prefix_length), -100, device=pixel_values.device),\n",
    "                caption_tokens\n",
    "            ], dim=1)\n",
    "        )\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def generate_caption(self, pixel_values, tokenizer, max_length=30, num_beams=None):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            B = pixel_values.size(0)\n",
    "            img_feats = self.get_image_features(pixel_values)                 \n",
    "            img_embeds = self.clip_to_gpt2(img_feats).unsqueeze(1)           \n",
    "            img_embeds = img_embeds.expand(B, self.prefix_length, img_embeds.size(-1))  \n",
    "    \n",
    "            generated = torch.zeros(B, 0, dtype=torch.long, device=pixel_values.device)\n",
    "            finished = torch.zeros(B, dtype=torch.bool, device=pixel_values.device)\n",
    "    \n",
    "            for _ in range(max_length):\n",
    "                if generated.size(1) > 0:\n",
    "                    cap_embeds = self.gpt2.transformer.wte(generated)         # [B, T, gpt2_dim]\n",
    "                    inputs_embeds = torch.cat([img_embeds, cap_embeds], dim=1)\n",
    "                else:\n",
    "                    inputs_embeds = img_embeds\n",
    "    \n",
    "                out = self.gpt2(inputs_embeds=inputs_embeds)\n",
    "                logits = out.logits[:, -1, :]                                 # [B, vocab]\n",
    "                next_token = torch.argmax(logits, dim=-1, keepdim=True)       # [B, 1]\n",
    "    \n",
    "\n",
    "                generated = torch.cat([generated, next_token], dim=1)         # [B, T+1]\n",
    "    \n",
    "\n",
    "                eos = (next_token.squeeze(1) == tokenizer.eos_token_id)       # [B]\n",
    "                finished |= eos\n",
    "    \n",
    "                if finished.all():\n",
    "                    break\n",
    "    \n",
    "            return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434f963",
   "metadata": {},
   "source": [
    "## ⚙️ Training Configuration\n",
    "\n",
    "Setting up the training pipeline with data loaders and model initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b2c3bc4-00e1-4611-8a35-3b970f24b045",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T13:46:52.891407Z",
     "iopub.status.busy": "2025-09-10T13:46:52.891105Z",
     "iopub.status.idle": "2025-09-10T13:47:09.718263Z",
     "shell.execute_reply": "2025-09-10T13:47:09.716978Z",
     "shell.execute_reply.started": "2025-09-10T13:46:52.891381Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06114d4475b46b3bbeeef8134458f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10de8ddf912e418494cc7f53947c24f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616aefc0d9674d12ab280e521e463c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a93e584db94f1a8510b5e6a4c2329e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254ecca3fd3d4694ae64361a80661883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1976e15fb20f4c77a865dc49a6432c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbc2ba3e992483eb4c24f91e1d302ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1243029aec94473aee6623276c0e271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d55e879dbf424aadefde1b01f4a852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77db0f501b12405280be81353e7565ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a739bc358174cf28540d49a3ceb17a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a3cb80f2d4480b87269b948a390b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2dd2171412e4cf2a118833474e06538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542f12a1b0a84819a9ca35802330fca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33acc6aaa8b040b68c3adad6c3a0fa31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72154d2912c46868b5a29d84094b0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\n",
      "Trying strategy 1: {'sep': '|'}\n",
      "✅ Successfully loaded with strategy 1\n",
      "Shape: (158915, 3), Columns: ['image_name', ' comment_number', ' comment']\n",
      "Dataset shape: (158915, 3)\n",
      "Columns: ['image_name', ' comment_number', ' comment']\n",
      "Original columns: ['image_name', ' comment_number', ' comment']\n",
      "Using columns 0 (image) and 2 (caption)\n",
      "Removed 1 rows with missing/empty captions\n",
      "Final dataset prepared with 158914 samples\n",
      "Sample - Image: 1000092795.jpg\n",
      "Sample - Caption:  Two young guys with shaggy hair look at their hands while hanging out in the yard ....\n",
      "Found 31783 images in /kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images\n",
      "Training samples: 143022\n",
      "Validation samples: 15892\n",
      "\n",
      "=== Testing Dataset ===\n",
      "✅ Batch pixel_values shape: torch.Size([16, 3, 224, 224])\n",
      "✅ Batch caption_tokens shape: torch.Size([16, 50])\n",
      "✅ Sample caption: A man cuts a cake while others crowd around him ....\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 5e-5,\n",
    "    'num_epochs': 2,\n",
    "    'warmup_steps': 1000,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'save_steps': 1000,\n",
    "    'eval_steps': 500,\n",
    "    'logging_steps': 100,\n",
    "}\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = ClipGPT2Model().to(device)\n",
    "\n",
    "full_dataset = Flickr30kDataset(\n",
    "    csv_file='/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv',\n",
    "    images_dir='/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images', \n",
    "    clip_processor=clip_processor,\n",
    "    gpt2_tokenizer=gpt2_tokenizer\n",
    ")\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "\n",
    "print(\"\\n=== Testing Dataset ===\")\n",
    "try:\n",
    "    sample = next(iter(train_loader))\n",
    "    print(f\"✅ Batch pixel_values shape: {sample['pixel_values'].shape}\")\n",
    "    print(f\"✅ Batch caption_tokens shape: {sample['caption_tokens'].shape}\")\n",
    "    print(f\"✅ Sample caption: {sample['caption_text'][0][:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1096e0af",
   "metadata": {},
   "source": [
    "### Data Loading & Model Setup\n",
    "\n",
    "Initializing processors, tokenizers, and creating train/validation splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17469653-1029-4e88-b09b-343239342ddb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T16:34:29.141747Z",
     "iopub.status.busy": "2025-09-10T16:34:29.140983Z",
     "iopub.status.idle": "2025-09-10T16:34:41.706084Z",
     "shell.execute_reply": "2025-09-10T16:34:41.704863Z",
     "shell.execute_reply.started": "2025-09-10T16:34:29.141710Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps: 17878\n",
      "Steps per epoch: 8939\n",
      "🚀 Starting 2-epoch training...\n",
      "Dataset: 143024 training samples\n",
      "Batch size: 16\n",
      "Learning rate: 5e-05\n",
      "\n",
      "==================================================\n",
      "--- EPOCH 1/2 ---\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceffa50ff95e4a9e8cfd3aa213a8a811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/8939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/402055894.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/402055894.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_grad_norm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m_no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_no_grad_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# prevent generators from being exhausted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   2628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m         \"\"\"\n\u001b[0;32m-> 2630\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2631\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2661\u001b[0m             \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_duplicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2662\u001b[0m         )\n\u001b[0;32m-> 2663\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2602\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m                     \u001b[0mmemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2605\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodule_prefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm  # Better for Kaggle notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=0.01)\n",
    "total_steps = len(train_loader) * config['num_epochs']  # Now uses 2 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=config['warmup_steps'], \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Steps per epoch: {len(train_loader)}\")\n",
    "sys.stdout.flush()  \n",
    "def train_epoch(model, train_loader, optimizer, scheduler, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}', \n",
    "                       dynamic_ncols=True, leave=True)\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        \n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        caption_tokens = batch['caption_tokens'].to(device)\n",
    "        \n",
    "       \n",
    "        outputs = model(pixel_values, caption_tokens)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (step + 1)\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'avg': f'{avg_loss:.4f}',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "        })\n",
    "        \n",
    "\n",
    "        if step % config['logging_steps'] == 0 and step > 0:\n",
    "            print(f\"\\n[Step {step:4d}] Loss: {loss.item():.4f}, Avg: {avg_loss:.4f}\")\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate_epoch(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc='Validating', \n",
    "                           dynamic_ncols=True, leave=True)\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            caption_tokens = batch['caption_tokens'].to(device)\n",
    "            \n",
    "            outputs = model(pixel_values, caption_tokens)\n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "            current_val_loss = total_loss / (len([b for b in progress_bar if True]) + 1)\n",
    "            progress_bar.set_postfix({'val_loss': f'{current_val_loss:.4f}'})\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"🚀 Starting 2-epoch training...\")\n",
    "print(f\"Dataset: {len(train_loader) * config['batch_size']} training samples\")\n",
    "print(f\"Batch size: {config['batch_size']}\")\n",
    "print(f\"Learning rate: {config['learning_rate']}\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "for epoch in range(config['num_epochs']):  # Will loop 2 times\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"--- EPOCH {epoch + 1}/{config['num_epochs']} ---\")\n",
    "    print(f\"{'='*50}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(f\"\\n📊 Epoch {epoch + 1} Training Results:\")\n",
    "    print(f\"   Final Training Loss: {train_loss:.4f}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "    print(\"🔍 Running validation...\")\n",
    "    sys.stdout.flush()\n",
    "    val_loss = validate_epoch(model, val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"📊 Epoch {epoch + 1} Validation Results:\")\n",
    "    print(f\"   Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"   Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if len(val_losses) > 1:\n",
    "        improvement = val_losses[-2] - val_losses[-1]\n",
    "        print(f\"   Improvement: {improvement:+.4f}\")\n",
    "    \n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'config': config\n",
    "        }, 'best_clip_gpt2_model.pth')\n",
    "        print(f\"🏆 NEW BEST MODEL SAVED!\")\n",
    "        print(f\"   Best Validation Loss: {val_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"   Previous best: {best_val_loss:.4f}\")\n",
    "    \n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"✅ 2-EPOCH TRAINING COMPLETED!\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "plt.plot(epochs, train_losses, 'b-o', label='Training Loss', linewidth=2, markersize=8)\n",
    "plt.plot(epochs, val_losses, 'r-s', label='Validation Loss', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Progress (2 Epochs)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(epochs)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "x = ['Epoch 1', 'Epoch 2']\n",
    "plt.bar([i-0.2 for i in range(1, len(train_losses)+1)], train_losses, \n",
    "        width=0.4, alpha=0.7, label='Train Loss', color='blue')\n",
    "plt.bar([i+0.2 for i in range(1, len(val_losses)+1)], val_losses, \n",
    "        width=0.4, alpha=0.7, label='Val Loss', color='red')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Loss Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.xticks(range(1, len(train_losses)+1), x)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📈 Training curves displayed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47101779-d6b4-4518-a844-8f760cf7723a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T16:12:21.561938Z",
     "iopub.status.busy": "2025-09-10T16:12:21.561264Z",
     "iopub.status.idle": "2025-09-10T16:12:24.315749Z",
     "shell.execute_reply": "2025-09-10T16:12:24.314957Z",
     "shell.execute_reply.started": "2025-09-10T16:12:21.561906Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual checkpoint saved: ckpt_after_epoch1.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.save({\n",
    "    'epoch': 0,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'train_loss': train_losses[-1] if len(train_losses) > 0 else None,\n",
    "    'val_loss': val_losses[-1] if len(val_losses) > 0 else None,\n",
    "    'config': config\n",
    "}, 'ckpt_after_epoch1.pth')\n",
    "\n",
    "print(\"Manual checkpoint saved: ckpt_after_epoch1.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a955c7",
   "metadata": {},
   "source": [
    "### Model Checkpointing\n",
    "\n",
    "Saving model states for resuming training later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90282cac-e454-4acc-9314-be61bd201eb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T16:35:04.455114Z",
     "iopub.status.busy": "2025-09-10T16:35:04.454837Z",
     "iopub.status.idle": "2025-09-10T16:35:04.459544Z",
     "shell.execute_reply": "2025-09-10T16:35:04.458886Z",
     "shell.execute_reply.started": "2025-09-10T16:35:04.455093Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /kaggle/working\n",
      "Exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('CWD:', os.getcwd())\n",
    "print('Exists:', os.path.exists('ckpt_after_epoch1.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d14c2c92-69cd-43f2-994a-c40fe554ada0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T16:35:05.835513Z",
     "iopub.status.busy": "2025-09-10T16:35:05.835221Z",
     "iopub.status.idle": "2025-09-10T16:35:09.731154Z",
     "shell.execute_reply": "2025-09-10T16:35:09.730411Z",
     "shell.execute_reply.started": "2025-09-10T16:35:05.835493Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded epoch index: 0\n",
      "Train loss saved: 0.7523651102776362\n",
      "Val loss saved: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ckpt_path = '/kaggle/working/ckpt_after_epoch1.pth'\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "model = ClipGPT2Model().to(device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded epoch index:\", ckpt.get('epoch'))\n",
    "print(\"Train loss saved:\", ckpt.get('train_loss'))\n",
    "print(\"Val loss saved:\", ckpt.get('val_loss'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0128a88-9b9f-4c9a-914e-528ebae8f0f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T16:35:09.732331Z",
     "iopub.status.busy": "2025-09-10T16:35:09.732043Z",
     "iopub.status.idle": "2025-09-10T16:35:09.739027Z",
     "shell.execute_reply": "2025-09-10T16:35:09.738277Z",
     "shell.execute_reply.started": "2025-09-10T16:35:09.732308Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "@torch.no_grad()\n",
    "def caption_image(image_path, model, clip_processor, tokenizer, device, max_length=30):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    pixel_values = inputs['pixel_values']\n",
    "    tokens = model.generate_caption(pixel_values, tokenizer, max_length=max_length)\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ce4fb39-cf6c-4711-b091-3144041b81ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T16:35:09.740670Z",
     "iopub.status.busy": "2025-09-10T16:35:09.740399Z",
     "iopub.status.idle": "2025-09-10T16:35:10.855665Z",
     "shell.execute_reply": "2025-09-10T16:35:10.854750Z",
     "shell.execute_reply.started": "2025-09-10T16:35:09.740649Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0]\n",
      "GT : A kid with Green shoes , red shorts , and a blue hat and shirt stands on a stretch of pavement .\n",
      "PR : A young boy in a red shirt and blue jeans is riding a skateboard .\n",
      "\n",
      "[1]\n",
      "GT : Construction men working on scaffolds .\n",
      "PR : Construction workers are working on a building .A man in a blue shirt is standing\n",
      "\n",
      "[2]\n",
      "GT : A woman in a beaded hat holding a sleeping baby .\n",
      "PR : A woman is holding a baby in a red coat .A woman is holding a\n",
      "\n",
      "[3]\n",
      "GT : A man eating olives and drinking .\n",
      "PR : A man with glasses and a green shirt is sitting at a table with a drink .\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_loader))\n",
    "images = batch['pixel_values'][:4].to(device)\n",
    "true_caps = batch['caption_text'][:4]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    gen_tokens = model.generate_caption(images, gpt2_tokenizer, max_length=30)\n",
    "pred_caps = [gpt2_tokenizer.decode(t, skip_special_tokens=True) for t in gen_tokens]\n",
    "\n",
    "for i, (gt, pr) in enumerate(zip(true_caps, pred_caps)):\n",
    "    print(f\"\\n[{i}]\")\n",
    "    print(\"GT :\", gt)\n",
    "    print(\"PR :\", pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6291b140-dde8-45f9-bbf4-3d5dd5d72059",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T17:02:15.327919Z",
     "iopub.status.busy": "2025-09-10T17:02:15.327028Z",
     "iopub.status.idle": "2025-09-10T17:02:16.848852Z",
     "shell.execute_reply": "2025-09-10T17:02:16.848056Z",
     "shell.execute_reply.started": "2025-09-10T17:02:15.327890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch index: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load('/kaggle/working/ckpt_after_epoch1.pth', map_location=device)\n",
    "\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
    "\n",
    "start_epoch = ckpt.get('epoch', 0) + 1 \n",
    "print(\"Resuming from epoch index:\", start_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1399b88f-66b9-4078-ab26-3e4a539980d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T17:02:30.176697Z",
     "iopub.status.busy": "2025-09-10T17:02:30.176204Z",
     "iopub.status.idle": "2025-09-10T17:56:54.854028Z",
     "shell.execute_reply": "2025-09-10T17:56:54.852796Z",
     "shell.execute_reply.started": "2025-09-10T17:02:30.176671Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EPOCH 2/2 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c7129f11114185995d6c5fb31bb39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/8939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step  100] Loss: 0.8123, Avg: 0.6774\n",
      "\n",
      "[Step  200] Loss: 0.5878, Avg: 0.6762\n",
      "\n",
      "[Step  300] Loss: 0.5588, Avg: 0.6779\n",
      "\n",
      "[Step  400] Loss: 0.6740, Avg: 0.6794\n",
      "\n",
      "[Step  500] Loss: 0.5406, Avg: 0.6799\n",
      "\n",
      "[Step  600] Loss: 0.6473, Avg: 0.6779\n",
      "\n",
      "[Step  700] Loss: 0.7479, Avg: 0.6782\n",
      "\n",
      "[Step  800] Loss: 0.6385, Avg: 0.6793\n",
      "\n",
      "[Step  900] Loss: 0.6327, Avg: 0.6802\n",
      "\n",
      "[Step 1000] Loss: 0.6947, Avg: 0.6805\n",
      "\n",
      "[Step 1100] Loss: 0.6506, Avg: 0.6801\n",
      "\n",
      "[Step 1200] Loss: 0.5248, Avg: 0.6812\n",
      "\n",
      "[Step 1300] Loss: 0.6622, Avg: 0.6805\n",
      "\n",
      "[Step 1400] Loss: 0.8951, Avg: 0.6806\n",
      "\n",
      "[Step 1500] Loss: 0.6997, Avg: 0.6825\n",
      "\n",
      "[Step 1600] Loss: 0.6088, Avg: 0.6821\n",
      "\n",
      "[Step 1700] Loss: 0.7029, Avg: 0.6828\n",
      "\n",
      "[Step 1800] Loss: 0.6418, Avg: 0.6846\n",
      "\n",
      "[Step 1900] Loss: 0.6646, Avg: 0.6840\n",
      "\n",
      "[Step 2000] Loss: 0.6708, Avg: 0.6830\n",
      "\n",
      "[Step 2100] Loss: 0.7133, Avg: 0.6831\n",
      "\n",
      "[Step 2200] Loss: 0.7303, Avg: 0.6834\n",
      "\n",
      "[Step 2300] Loss: 0.6080, Avg: 0.6829\n",
      "\n",
      "[Step 2400] Loss: 0.7446, Avg: 0.6823\n",
      "\n",
      "[Step 2500] Loss: 0.5653, Avg: 0.6825\n",
      "\n",
      "[Step 2600] Loss: 0.8701, Avg: 0.6819\n",
      "\n",
      "[Step 2700] Loss: 0.9779, Avg: 0.6823\n",
      "\n",
      "[Step 2800] Loss: 0.7024, Avg: 0.6821\n",
      "\n",
      "[Step 2900] Loss: 0.7830, Avg: 0.6822\n",
      "\n",
      "[Step 3000] Loss: 0.5406, Avg: 0.6818\n",
      "\n",
      "[Step 3100] Loss: 0.6991, Avg: 0.6814\n",
      "\n",
      "[Step 3200] Loss: 0.5243, Avg: 0.6813\n",
      "\n",
      "[Step 3300] Loss: 0.5349, Avg: 0.6812\n",
      "\n",
      "[Step 3400] Loss: 0.5628, Avg: 0.6811\n",
      "\n",
      "[Step 3500] Loss: 0.6483, Avg: 0.6813\n",
      "\n",
      "[Step 3600] Loss: 0.7379, Avg: 0.6815\n",
      "\n",
      "[Step 3700] Loss: 0.7337, Avg: 0.6814\n",
      "\n",
      "[Step 3800] Loss: 0.5890, Avg: 0.6812\n",
      "\n",
      "[Step 3900] Loss: 0.6784, Avg: 0.6812\n",
      "\n",
      "[Step 4000] Loss: 0.6211, Avg: 0.6814\n",
      "\n",
      "[Step 4100] Loss: 0.6730, Avg: 0.6817\n",
      "\n",
      "[Step 4200] Loss: 0.6261, Avg: 0.6818\n",
      "\n",
      "[Step 4300] Loss: 0.6511, Avg: 0.6822\n",
      "\n",
      "[Step 4400] Loss: 0.6523, Avg: 0.6821\n",
      "\n",
      "[Step 4500] Loss: 0.7012, Avg: 0.6818\n",
      "\n",
      "[Step 4600] Loss: 0.5856, Avg: 0.6818\n",
      "\n",
      "[Step 4700] Loss: 0.8000, Avg: 0.6815\n",
      "\n",
      "[Step 4800] Loss: 0.6266, Avg: 0.6814\n",
      "\n",
      "[Step 4900] Loss: 0.6851, Avg: 0.6812\n",
      "\n",
      "[Step 5000] Loss: 0.7268, Avg: 0.6809\n",
      "\n",
      "[Step 5100] Loss: 0.6271, Avg: 0.6808\n",
      "\n",
      "[Step 5200] Loss: 0.6588, Avg: 0.6807\n",
      "\n",
      "[Step 5300] Loss: 0.5727, Avg: 0.6808\n",
      "\n",
      "[Step 5400] Loss: 0.6495, Avg: 0.6810\n",
      "\n",
      "[Step 5500] Loss: 0.4747, Avg: 0.6810\n",
      "\n",
      "[Step 5600] Loss: 0.7821, Avg: 0.6811\n",
      "\n",
      "[Step 5700] Loss: 0.5467, Avg: 0.6810\n",
      "\n",
      "[Step 5800] Loss: 0.6756, Avg: 0.6807\n",
      "\n",
      "[Step 5900] Loss: 0.5620, Avg: 0.6808\n",
      "\n",
      "[Step 6000] Loss: 0.6421, Avg: 0.6809\n",
      "\n",
      "[Step 6100] Loss: 0.7076, Avg: 0.6808\n",
      "\n",
      "[Step 6200] Loss: 0.6227, Avg: 0.6808\n",
      "\n",
      "[Step 6300] Loss: 0.6747, Avg: 0.6807\n",
      "\n",
      "[Step 6400] Loss: 0.4836, Avg: 0.6808\n",
      "\n",
      "[Step 6500] Loss: 0.6418, Avg: 0.6808\n",
      "\n",
      "[Step 6600] Loss: 0.6863, Avg: 0.6808\n",
      "\n",
      "[Step 6700] Loss: 0.6598, Avg: 0.6809\n",
      "\n",
      "[Step 6800] Loss: 0.6690, Avg: 0.6810\n",
      "\n",
      "[Step 6900] Loss: 0.8168, Avg: 0.6812\n",
      "\n",
      "[Step 7000] Loss: 0.7452, Avg: 0.6812\n",
      "\n",
      "[Step 7100] Loss: 0.6412, Avg: 0.6813\n",
      "\n",
      "[Step 7200] Loss: 0.7247, Avg: 0.6814\n",
      "\n",
      "[Step 7300] Loss: 0.7310, Avg: 0.6814\n",
      "\n",
      "[Step 7400] Loss: 0.6511, Avg: 0.6813\n",
      "\n",
      "[Step 7500] Loss: 0.4884, Avg: 0.6815\n",
      "\n",
      "[Step 7600] Loss: 0.5865, Avg: 0.6815\n",
      "\n",
      "[Step 7700] Loss: 0.6655, Avg: 0.6815\n",
      "\n",
      "[Step 7800] Loss: 0.7396, Avg: 0.6814\n",
      "\n",
      "[Step 7900] Loss: 0.6656, Avg: 0.6817\n",
      "\n",
      "[Step 8000] Loss: 0.7478, Avg: 0.6816\n",
      "\n",
      "[Step 8100] Loss: 0.7182, Avg: 0.6817\n",
      "\n",
      "[Step 8200] Loss: 0.7429, Avg: 0.6818\n",
      "\n",
      "[Step 8300] Loss: 0.7562, Avg: 0.6818\n",
      "\n",
      "[Step 8400] Loss: 0.6207, Avg: 0.6817\n",
      "\n",
      "[Step 8500] Loss: 0.6151, Avg: 0.6818\n",
      "\n",
      "[Step 8600] Loss: 0.6939, Avg: 0.6817\n",
      "\n",
      "[Step 8700] Loss: 0.6036, Avg: 0.6818\n",
      "\n",
      "[Step 8800] Loss: 0.6973, Avg: 0.6819\n",
      "\n",
      "[Step 8900] Loss: 0.6753, Avg: 0.6818\n",
      "Train Loss: 0.6818\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8147fc9a3ee1467ab88156dfc1523840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/2650870345.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train Loss: {train_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Val Loss: {val_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/402055894.py\u001b[0m in \u001b[0;36mvalidate_epoch\u001b[0;34m(model, val_loader)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# Show current validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mcurrent_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf'{current_val_loss:.4f}'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/402055894.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# Show current validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mcurrent_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf'{current_val_loss:.4f}'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;31m# (note: keep this check outside the loop for performance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, config['num_epochs']):\n",
    "    print(f\"\\n--- EPOCH {epoch + 1}/{config['num_epochs']} ---\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, epoch)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    val_loss = validate_epoch(model, val_loader)\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'config': config\n",
    "        }, 'best_clip_gpt2_model.pth')\n",
    "        print(\"🏆 NEW BEST MODEL SAVED!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4f8e092-e1cd-42aa-b72c-f9e38f7a63f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T17:57:01.434007Z",
     "iopub.status.busy": "2025-09-10T17:57:01.433672Z",
     "iopub.status.idle": "2025-09-10T17:57:03.945023Z",
     "shell.execute_reply": "2025-09-10T17:57:03.944377Z",
     "shell.execute_reply.started": "2025-09-10T17:57:01.433984Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual checkpoint saved to: /kaggle/working/ckpt_after_epoch2.pth | Exists: True\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "\n",
    "train_loss_last = train_losses[-1] if 'train_losses' in globals() and len(train_losses)>0 else None\n",
    "val_loss_last   = val_losses[-1] if 'val_losses'   in globals() and len(val_losses)>0 else None\n",
    "\n",
    "save_path = '/kaggle/working/ckpt_after_epoch2.pth'\n",
    "torch.save({\n",
    "    'epoch': 1,  # epoch index for epoch 2\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'train_loss': train_loss_last,\n",
    "    'val_loss': val_loss_last,\n",
    "    'config': config\n",
    "}, save_path)\n",
    "\n",
    "print('Manual checkpoint saved to:', save_path, '| Exists:', os.path.exists(save_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98a8f980-9d01-42f3-858c-fd94be3a67e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T17:57:58.873768Z",
     "iopub.status.busy": "2025-09-10T17:57:58.873453Z",
     "iopub.status.idle": "2025-09-10T17:57:59.818347Z",
     "shell.execute_reply": "2025-09-10T17:57:59.817492Z",
     "shell.execute_reply.started": "2025-09-10T17:57:58.873748Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Keys: ['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'train_loss', 'val_loss', 'config']\n",
      "Epoch index: 1\n",
      "Train loss: None\n",
      "Val loss: None\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "path = '/kaggle/working/ckpt_after_epoch2.pth'\n",
    "print('Exists:', os.path.exists(path))\n",
    "ckpt = torch.load(path, map_location='cpu')\n",
    "print('Keys:', list(ckpt.keys()))\n",
    "print('Epoch index:', ckpt.get('epoch'))  \n",
    "print('Train loss:', ckpt.get('train_loss'))\n",
    "print('Val loss:', ckpt.get('val_loss'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06c80ae1-1cab-4304-9de1-09b861ac760b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T18:00:55.728695Z",
     "iopub.status.busy": "2025-09-10T18:00:55.728118Z",
     "iopub.status.idle": "2025-09-10T18:00:59.450446Z",
     "shell.execute_reply": "2025-09-10T18:00:59.449678Z",
     "shell.execute_reply.started": "2025-09-10T18:00:55.728669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully for evaluation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, GPT2Tokenizer\n",
    "\n",
    "ckpt = torch.load('/kaggle/working/ckpt_after_epoch2.pth', map_location=device)\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "model = ClipGPT2Model().to(device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully for evaluation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1adb6",
   "metadata": {},
   "source": [
    "## 📊 Model Evaluation\n",
    "\n",
    "Evaluating the model using BLEU scores and other metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05fc7816-2417-410a-8eab-8c6fafedca73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T18:01:06.222934Z",
     "iopub.status.busy": "2025-09-10T18:01:06.222396Z",
     "iopub.status.idle": "2025-09-10T18:01:52.221861Z",
     "shell.execute_reply": "2025-09-10T18:01:52.221017Z",
     "shell.execute_reply.started": "2025-09-10T18:01:06.222910Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BLEU Score Results ===\n",
      "BLEU-1: 0.2631\n",
      "BLEU-2: 0.1479\n",
      "BLEU-3: 0.0875\n",
      "BLEU-4: 0.0517\n",
      "Evaluated on 1000 samples\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_bleu_scores(model, val_loader, tokenizer, device, num_samples=1000):\n",
    "    model.eval()\n",
    "    references = []  \n",
    "    candidates = []  \n",
    "    \n",
    "    sample_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            true_caps = batch['caption_text']\n",
    "            \n",
    "            gen_tokens = model.generate_caption(pixel_values, tokenizer, max_length=30)\n",
    "            pred_caps = [tokenizer.decode(t, skip_special_tokens=True) for t in gen_tokens]\n",
    "            \n",
    "            for true_cap, pred_cap in zip(true_caps, pred_caps):\n",
    "                references.append([true_cap.split()])  # Reference as list of words\n",
    "                candidates.append(pred_cap.split())     # Candidate as list of words\n",
    "                sample_count += 1\n",
    "                \n",
    "                if sample_count >= num_samples:\n",
    "                    break\n",
    "    \n",
    "    bleu1 = corpus_bleu(references, candidates, weights=(1, 0, 0, 0))\n",
    "    bleu2 = corpus_bleu(references, candidates, weights=(0.5, 0.5, 0, 0))  \n",
    "    bleu3 = corpus_bleu(references, candidates, weights=(0.33, 0.33, 0.33, 0))\n",
    "    bleu4 = corpus_bleu(references, candidates, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    return {\n",
    "        'BLEU-1': bleu1,\n",
    "        'BLEU-2': bleu2, \n",
    "        'BLEU-3': bleu3,\n",
    "        'BLEU-4': bleu4,\n",
    "        'samples_evaluated': len(candidates)\n",
    "    }\n",
    "\n",
    "results = evaluate_bleu_scores(model, val_loader, gpt2_tokenizer, device)\n",
    "print(\"=== BLEU Score Results ===\")\n",
    "for metric, score in results.items():\n",
    "    if metric != 'samples_evaluated':\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "print(f\"Evaluated on {results['samples_evaluated']} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d581cd",
   "metadata": {},
   "source": [
    "### Advanced Metrics\n",
    "\n",
    "Computing METEOR, ROUGE, and CIDEr scores for comprehensive evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65b8f8ac-c3da-490f-8d7b-ac95e00d031d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T18:02:53.083660Z",
     "iopub.status.busy": "2025-09-10T18:02:53.083313Z",
     "iopub.status.idle": "2025-09-10T18:03:33.911383Z",
     "shell.execute_reply": "2025-09-10T18:03:33.910440Z",
     "shell.execute_reply.started": "2025-09-10T18:02:53.083634Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocoevalcap\n",
      "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap) (2.0.10)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools>=2.0.2->pycocoevalcap) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools>=2.0.2->pycocoevalcap) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pycocotools>=2.0.2->pycocoevalcap) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pycocotools>=2.0.2->pycocoevalcap) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pycocotools>=2.0.2->pycocoevalcap) (2024.2.0)\n",
      "Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pycocoevalcap\n",
      "Successfully installed pycocoevalcap-1.2\n",
      "{'testlen': 8882, 'reflen': 6646, 'guess': [8882, 8382, 7882, 7382], 'correct': [2306, 686, 226, 94]}\n",
      "ratio: 1.3364429732167715\n",
      "=== Comprehensive Evaluation Results ===\n",
      "Bleu_1: 0.2596\n",
      "Bleu_2: 0.1458\n",
      "Bleu_3: 0.0848\n",
      "Bleu_4: 0.0528\n",
      "METEOR: 0.1382\n",
      "ROUGE_L: 0.2742\n",
      "CIDEr: 0.3776\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocoevalcap\n",
    "\n",
    "# Comprehensive evaluation function\n",
    "def comprehensive_evaluation(model, val_loader, tokenizer, device, num_samples=500):\n",
    "    from pycocoevalcap.bleu.bleu import Bleu\n",
    "    from pycocoevalcap.meteor.meteor import Meteor\n",
    "    from pycocoevalcap.rouge.rouge import Rouge\n",
    "    from pycocoevalcap.cider.cider import Cider\n",
    "    \n",
    "    model.eval()\n",
    "    gts = {}  \n",
    "    res = {}  \n",
    "    img_id = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            if img_id >= num_samples:\n",
    "                break\n",
    "                \n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            true_caps = batch['caption_text']\n",
    "            \n",
    "            gen_tokens = model.generate_caption(pixel_values, tokenizer, max_length=30)\n",
    "            pred_caps = [tokenizer.decode(t, skip_special_tokens=True) for t in gen_tokens]\n",
    "            \n",
    "            for true_cap, pred_cap in zip(true_caps, pred_caps):\n",
    "                gts[img_id] = [true_cap]\n",
    "                res[img_id] = [pred_cap]\n",
    "                img_id += 1\n",
    "                \n",
    "                if img_id >= num_samples:\n",
    "                    break\n",
    "    scorers = [\n",
    "        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "        (Meteor(), \"METEOR\"),\n",
    "        (Rouge(), \"ROUGE_L\"),\n",
    "        (Cider(), \"CIDEr\")\n",
    "    ]\n",
    "    \n",
    "    eval_results = {}\n",
    "    for scorer, method in scorers:\n",
    "        score, scores = scorer.compute_score(gts, res)\n",
    "        if isinstance(method, list):\n",
    "            for sc, scs, m in zip(score, scores, method):\n",
    "                eval_results[m] = sc\n",
    "        else:\n",
    "            eval_results[method] = score\n",
    "    \n",
    "    return eval_results\n",
    "try:\n",
    "    comp_results = comprehensive_evaluation(model, val_loader, gpt2_tokenizer, device)\n",
    "    print(\"=== Comprehensive Evaluation Results ===\")\n",
    "    for metric, score in comp_results.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "except ImportError:\n",
    "    print(\"pycocoevalcap not available. Install it for comprehensive metrics.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31fc1cd9-a845-478c-8efc-9af523e04e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T18:06:47.004138Z",
     "iopub.status.busy": "2025-09-10T18:06:47.003402Z",
     "iopub.status.idle": "2025-09-10T18:06:48.391190Z",
     "shell.execute_reply": "2025-09-10T18:06:48.390504Z",
     "shell.execute_reply.started": "2025-09-10T18:06:47.004111Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded and ready for caption generation!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, GPT2Tokenizer\n",
    "ckpt = torch.load('/kaggle/working/ckpt_after_epoch2.pth', map_location=device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Model loaded and ready for caption generation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68a9da",
   "metadata": {},
   "source": [
    "## 🎯 Sample Caption Generation\n",
    "\n",
    "Testing the trained model on sample images from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9aa41e4b-3cba-448d-9f89-e9d5c1bca11f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T18:06:54.099137Z",
     "iopub.status.busy": "2025-09-10T18:06:54.098427Z",
     "iopub.status.idle": "2025-09-10T18:06:54.526917Z",
     "shell.execute_reply": "2025-09-10T18:06:54.526079Z",
     "shell.execute_reply.started": "2025-09-10T18:06:54.099113Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Generated Captions:\n",
      "[1] A man and a woman are walking down a sidewalk .\n",
      "[2] A little girl is sitting on a wooden bench outside .\n",
      "[3] A man is standing on a ladder fixing a window .\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption_for_image(img_path, model, clip_processor, tokenizer, device, max_length=30):\n",
    "    \"\"\"Generate caption for a single image\"\"\"\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    inputs = clip_processor(images=image, return_tensors='pt').to(device)\n",
    "    pixel_values = inputs['pixel_values']\n",
    "    tokens = model.generate_caption(pixel_values, tokenizer, max_length=max_length)\n",
    "    caption = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "test_images = [\n",
    "    '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/1000092795.jpg',\n",
    "    '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/1000268201.jpg',\n",
    "    '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/1000344755.jpg'\n",
    "]\n",
    "\n",
    "print(\"🎯 Generated Captions:\")\n",
    "for i, img_path in enumerate(test_images):\n",
    "    caption = generate_caption_for_image(img_path, model, clip_processor, gpt2_tokenizer, device)\n",
    "    print(f\"[{i+1}] {caption}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c72bacc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d73b1-02c0-4f13-b539-83e9caa072ad",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f6def-1e87-4d69-a61f-d1e5cdf216d2",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2bcb6-06c6-4978-8af4-977df638566e",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122fa61d-f41f-41ec-80d5-da4b9a2d5d86",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d53e29b-e108-44ec-9762-aaad1d104d0e",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9461e1-4fef-4ced-a926-5261a836cd57",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ce628-2b6d-42f6-8607-0c0daed857f9",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23982a0-b66a-4475-8896-3bfff4106b09",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e0ea9-3d63-495a-8365-edcf1dcf1ea0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63008b34-8b03-4b2b-8400-15a98b7ce353",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01497d78-d894-4e6e-af6e-03ed615710b7",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 31296,
     "sourceId": 39911,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
