{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b472513a",
   "metadata": {},
   "source": [
    "# CNN-LSTM Image Captioning Model\n",
    "\n",
    "This notebook implements an image captioning system using CNN for feature extraction and LSTM for sequence generation. The model is trained on the Flickr30k dataset to generate descriptive captions for images.\n",
    "\n",
    "## üìÅ Dataset Overview\n",
    "\n",
    "Exploring the Flickr30k dataset structure and loading the images and captions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d87966b",
   "metadata": {},
   "source": [
    "### Image Data Exploration\n",
    "\n",
    "First, let's check how many images we have in our dataset directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf04e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images found: 31783\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_dir = \"flickr30k_images\"\n",
    "image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "print(f\"Total images found: {len(image_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c2ef83",
   "metadata": {},
   "source": [
    "### Caption Data Loading\n",
    "\n",
    "Loading and exploring the caption data from the CSV file. Each image has multiple captions (usually 5 per image).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c6c7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            image   caption_number  \\\n",
      "0      image_name   comment_number   \n",
      "1  1000092795.jpg                0   \n",
      "2  1000092795.jpg                1   \n",
      "3  1000092795.jpg                2   \n",
      "4  1000092795.jpg                3   \n",
      "\n",
      "                                             caption  \n",
      "0                                            comment  \n",
      "1   Two young guys with shaggy hair look at their...  \n",
      "2   Two young , White males are outside near many...  \n",
      "3   Two men in green shirts are standing in a yard .  \n",
      "4       A man in a blue shirt standing in a garden .  \n",
      "Total captions: 158916\n",
      "Unique images: 31784\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = os.path.join(\"flickr30k_images\", \"results.csv\")\n",
    "df = pd.read_csv(csv_path, delimiter=\"|\", header=None, names=[\"image\", \"caption_number\", \"caption\"])\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "print(\"Total captions:\", len(df))\n",
    "print(\"Unique images:\", df['image'].nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc39cc",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Cleaning up the dataset by removing header rows and preparing the data for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109cdae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total captions after cleanup: 158915\n",
      "Unique images after cleanup: 31783\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(index=0).reset_index(drop=True)\n",
    "\n",
    "print(\"Total captions after cleanup:\", len(df))\n",
    "print(\"Unique images after cleanup:\", df['image'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c65f8846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption_number</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Two young , White males are outside near many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image caption_number  \\\n",
       "0  1000092795.jpg              0   \n",
       "1  1000092795.jpg              1   \n",
       "2  1000092795.jpg              2   \n",
       "3  1000092795.jpg              3   \n",
       "4  1000092795.jpg              4   \n",
       "\n",
       "                                             caption  \n",
       "0   Two young guys with shaggy hair look at their...  \n",
       "1   Two young , White males are outside near many...  \n",
       "2   Two men in green shirts are standing in a yard .  \n",
       "3       A man in a blue shirt standing in a garden .  \n",
       "4            Two friends enjoy time spent together .  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Two young guys with shaggy hair look at their hands while hanging out in the yard .', ' Two young , White males are outside near many bushes .', ' Two men in green shirts are standing in a yard .', ' A man in a blue shirt standing in a garden .', ' Two friends enjoy time spent together .']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "image_captions = defaultdict(list)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    image_captions[row[\"image\"]].append(row[\"caption\"])\n",
    "\n",
    "# Example check\n",
    "print(image_captions[\"1000092795.jpg\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3107c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> two young guys with shaggy hair look at their hands while hanging out in the yard <end>', '<start> two young  white males are outside near many bushes <end>', '<start> two men in green shirts are standing in a yard <end>', '<start> a man in a blue shirt standing in a garden <end>', '<start> two friends enjoy time spent together <end>']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_caption(caption):\n",
    "    if not isinstance(caption, str):\n",
    "        return None\n",
    "    caption = caption.lower()\n",
    "    caption = re.sub(r\"[^a-z ]\", \"\", caption) \n",
    "    caption = caption.strip()\n",
    "    return f\"<start> {caption} <end>\"\n",
    "\n",
    "for img, caps in image_captions.items():\n",
    "    cleaned = [clean_caption(c) for c in caps]\n",
    "    image_captions[img] = [c for c in cleaned if c is not None]\n",
    "\n",
    "# Example check\n",
    "print(image_captions[\"1000092795.jpg\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c221448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing captions: 0\n"
     ]
    }
   ],
   "source": [
    "missing_count = sum(1 for caps in image_captions.values() for c in caps if c is None)\n",
    "print(\"Missing captions:\", missing_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1def1b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 31783\n",
      "Min captions per image: 4\n",
      "Max captions per image: 5\n",
      "All images have exactly 5 captions? False\n"
     ]
    }
   ],
   "source": [
    "# Total images\n",
    "print(\"Total images:\", len(image_captions))\n",
    "\n",
    "# Check how many captions per image\n",
    "caption_counts = [len(caps) for caps in image_captions.values()]\n",
    "print(\"Min captions per image:\", min(caption_counts))\n",
    "print(\"Max captions per image:\", max(caption_counts))\n",
    "\n",
    "# Check if all are 5\n",
    "all_five = all(count == 5 for count in caption_counts)\n",
    "print(\"All images have exactly 5 captions?\", all_five)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eda18615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset size: 31782\n"
     ]
    }
   ],
   "source": [
    "# Keep only images with 5 captions\n",
    "image_captions = {img: caps for img, caps in image_captions.items() if len(caps) == 5}\n",
    "\n",
    "print(\"Cleaned dataset size:\", len(image_captions))  # should be slightly less than 31783\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd9db52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption_number</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Two young , White males are outside near many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10002456.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Several men in hard hats are operating a gian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10002456.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Workers look down from up above on a piece of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10002456.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Two men working on a machine wearing hard hats .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10002456.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>Four men on top of a tall structure .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10002456.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Three men on a large rig .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            image caption_number  \\\n",
       "0  1000092795.jpg              0   \n",
       "1  1000092795.jpg              1   \n",
       "2  1000092795.jpg              2   \n",
       "3  1000092795.jpg              3   \n",
       "4  1000092795.jpg              4   \n",
       "5    10002456.jpg              0   \n",
       "6    10002456.jpg              1   \n",
       "7    10002456.jpg              2   \n",
       "8    10002456.jpg              3   \n",
       "9    10002456.jpg              4   \n",
       "\n",
       "                                             caption  \n",
       "0   Two young guys with shaggy hair look at their...  \n",
       "1   Two young , White males are outside near many...  \n",
       "2   Two men in green shirts are standing in a yard .  \n",
       "3       A man in a blue shirt standing in a garden .  \n",
       "4            Two friends enjoy time spent together .  \n",
       "5   Several men in hard hats are operating a gian...  \n",
       "6   Workers look down from up above on a piece of...  \n",
       "7   Two men working on a machine wearing hard hats .  \n",
       "8              Four men on top of a tall structure .  \n",
       "9                         Three men on a large rig .  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "821627b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"caption\"])\n",
    "\n",
    "df[\"caption\"] = df[\"caption\"].apply(lambda x: \"<start> \" + x.lower() + \" <end>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03f4f9e",
   "metadata": {},
   "source": [
    "## üìù Vocabulary Construction\n",
    "\n",
    "Building vocabulary from captions and preparing text preprocessing utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca1fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def tokenizer(self, text):\n",
    "        return nltk.tokenize.word_tokenize(text.lower())\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                frequencies[word] += 1\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized = self.tokenizer(text)\n",
    "        return [\n",
    "            self.stoi.get(token, self.stoi[\"<unk>\"])\n",
    "            for token in tokenized\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51d715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sushi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/sushi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8fd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 7738\n"
     ]
    }
   ],
   "source": [
    "captions = df[\"caption\"].tolist()\n",
    "\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "vocab.build_vocabulary(captions)\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49e0f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, vocab, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.df.loc[idx, \"caption\"]\n",
    "        img_id = self.df.loc[idx, \"image\"]\n",
    "        img_path = os.path.join(self.image_dir, img_id)\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Numericalize caption\n",
    "        tokens = [self.vocab.stoi[\"<start>\"]] + \\\n",
    "                 self.vocab.numericalize(caption) + \\\n",
    "                 [self.vocab.stoi[\"<end>\"]]\n",
    "\n",
    "        return image, torch.tensor(tokens)\n",
    "\n",
    "# Pad captions in batch\n",
    "def collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    captions = pad_sequence(captions, batch_first=True, padding_value=0)\n",
    "    return images, captions\n",
    "\n",
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899bed48",
   "metadata": {},
   "source": [
    "## üèóÔ∏è CNN-LSTM Architecture\n",
    "\n",
    "Defining the CNN-LSTM model that combines visual features with sequence generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dd9bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        modules = list(resnet.children())[:-1]  # remove fc\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        return self.bn(self.linear(features))\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.dropout(self.embed(captions[:, :-1]))\n",
    "        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        hiddens, _ = self.lstm(inputs)\n",
    "        return self.linear(hiddens)\n",
    "\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c9624",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Training Setup\n",
    "\n",
    "Setting up data loaders, loss functions, and training parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d07b80a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Params\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab)\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "lr = 3e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data\n",
    "dataset = FlickrDataset(df, \"flickr30k_images\", vocab, transform)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Model\n",
    "model = ImageCaptioningModel(embed_size, hidden_size, vocab_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore <pad>\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b9e66",
   "metadata": {},
   "source": [
    "## üöÄ Model Training\n",
    "\n",
    "Training the CNN-LSTM model with progress tracking and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c11a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [0/2484], Loss: 8.9638\n",
      "Epoch [1/5], Step [100/2484], Loss: 3.9689\n",
      "Epoch [1/5], Step [200/2484], Loss: 3.6687\n",
      "Epoch [1/5], Step [300/2484], Loss: 3.4072\n",
      "Epoch [1/5], Step [400/2484], Loss: 3.5085\n",
      "Epoch [1/5], Step [500/2484], Loss: 3.4992\n",
      "Epoch [1/5], Step [600/2484], Loss: 3.3269\n",
      "Epoch [1/5], Step [700/2484], Loss: 3.3737\n",
      "Epoch [1/5], Step [800/2484], Loss: 3.2639\n",
      "Epoch [1/5], Step [900/2484], Loss: 3.3896\n",
      "Epoch [1/5], Step [1000/2484], Loss: 2.9495\n",
      "Epoch [1/5], Step [1100/2484], Loss: 3.2429\n",
      "Epoch [1/5], Step [1200/2484], Loss: 3.1634\n",
      "Epoch [1/5], Step [1300/2484], Loss: 3.1939\n",
      "Epoch [1/5], Step [1400/2484], Loss: 3.0893\n",
      "Epoch [1/5], Step [1500/2484], Loss: 3.1787\n",
      "Epoch [1/5], Step [1600/2484], Loss: 2.9403\n",
      "Epoch [1/5], Step [1700/2484], Loss: 3.1467\n",
      "Epoch [1/5], Step [1800/2484], Loss: 3.1561\n",
      "Epoch [1/5], Step [1900/2484], Loss: 3.1182\n",
      "Epoch [1/5], Step [2000/2484], Loss: 3.1382\n",
      "Epoch [1/5], Step [2100/2484], Loss: 3.3731\n",
      "Epoch [1/5], Step [2200/2484], Loss: 2.9499\n",
      "Epoch [1/5], Step [2300/2484], Loss: 2.9741\n",
      "Epoch [1/5], Step [2400/2484], Loss: 3.1485\n",
      "Epoch [1/5], Avg Loss: 3.2919\n",
      "Epoch [2/5], Step [0/2484], Loss: 2.9704\n",
      "Epoch [2/5], Step [100/2484], Loss: 3.0237\n",
      "Epoch [2/5], Step [200/2484], Loss: 2.9685\n",
      "Epoch [2/5], Step [300/2484], Loss: 3.1237\n",
      "Epoch [2/5], Step [400/2484], Loss: 2.7890\n",
      "Epoch [2/5], Step [500/2484], Loss: 2.8980\n",
      "Epoch [2/5], Step [600/2484], Loss: 3.0609\n",
      "Epoch [2/5], Step [700/2484], Loss: 2.7729\n",
      "Epoch [2/5], Step [800/2484], Loss: 2.8703\n",
      "Epoch [2/5], Step [900/2484], Loss: 3.0609\n",
      "Epoch [2/5], Step [1000/2484], Loss: 2.7365\n",
      "Epoch [2/5], Step [1100/2484], Loss: 3.1270\n",
      "Epoch [2/5], Step [1200/2484], Loss: 2.9311\n",
      "Epoch [2/5], Step [1300/2484], Loss: 2.7044\n",
      "Epoch [2/5], Step [1400/2484], Loss: 3.0347\n",
      "Epoch [2/5], Step [1500/2484], Loss: 2.8554\n",
      "Epoch [2/5], Step [1600/2484], Loss: 2.9381\n",
      "Epoch [2/5], Step [1700/2484], Loss: 2.7445\n",
      "Epoch [2/5], Step [1800/2484], Loss: 3.0270\n",
      "Epoch [2/5], Step [1900/2484], Loss: 2.7822\n",
      "Epoch [2/5], Step [2000/2484], Loss: 2.8932\n",
      "Epoch [2/5], Step [2100/2484], Loss: 3.0288\n",
      "Epoch [2/5], Step [2200/2484], Loss: 2.7230\n",
      "Epoch [2/5], Step [2300/2484], Loss: 2.9287\n",
      "Epoch [2/5], Step [2400/2484], Loss: 2.8115\n",
      "Epoch [2/5], Avg Loss: 2.8935\n",
      "Epoch [3/5], Step [0/2484], Loss: 2.8085\n",
      "Epoch [3/5], Step [100/2484], Loss: 3.0370\n",
      "Epoch [3/5], Step [200/2484], Loss: 2.8328\n",
      "Epoch [3/5], Step [300/2484], Loss: 2.7909\n",
      "Epoch [3/5], Step [400/2484], Loss: 2.8469\n",
      "Epoch [3/5], Step [500/2484], Loss: 2.6014\n",
      "Epoch [3/5], Step [600/2484], Loss: 2.8136\n",
      "Epoch [3/5], Step [700/2484], Loss: 2.4660\n",
      "Epoch [3/5], Step [800/2484], Loss: 2.6650\n",
      "Epoch [3/5], Step [900/2484], Loss: 2.8713\n",
      "Epoch [3/5], Step [1000/2484], Loss: 2.8199\n",
      "Epoch [3/5], Step [1100/2484], Loss: 2.8048\n",
      "Epoch [3/5], Step [1200/2484], Loss: 2.5415\n",
      "Epoch [3/5], Step [1300/2484], Loss: 2.8796\n",
      "Epoch [3/5], Step [1400/2484], Loss: 2.7899\n",
      "Epoch [3/5], Step [1500/2484], Loss: 2.6134\n",
      "Epoch [3/5], Step [1600/2484], Loss: 2.8651\n",
      "Epoch [3/5], Step [1700/2484], Loss: 2.7490\n",
      "Epoch [3/5], Step [1800/2484], Loss: 2.6619\n",
      "Epoch [3/5], Step [1900/2484], Loss: 2.8258\n",
      "Epoch [3/5], Step [2000/2484], Loss: 2.8222\n",
      "Epoch [3/5], Step [2100/2484], Loss: 2.5775\n",
      "Epoch [3/5], Step [2200/2484], Loss: 2.7818\n",
      "Epoch [3/5], Step [2300/2484], Loss: 2.7658\n",
      "Epoch [3/5], Step [2400/2484], Loss: 2.5841\n",
      "Epoch [3/5], Avg Loss: 2.7505\n",
      "Epoch [4/5], Step [0/2484], Loss: 2.8039\n",
      "Epoch [4/5], Step [100/2484], Loss: 2.7248\n",
      "Epoch [4/5], Step [200/2484], Loss: 2.6873\n",
      "Epoch [4/5], Step [300/2484], Loss: 2.8050\n",
      "Epoch [4/5], Step [400/2484], Loss: 2.6661\n",
      "Epoch [4/5], Step [500/2484], Loss: 2.6832\n",
      "Epoch [4/5], Step [600/2484], Loss: 2.5631\n",
      "Epoch [4/5], Step [700/2484], Loss: 2.4133\n",
      "Epoch [4/5], Step [800/2484], Loss: 2.8810\n",
      "Epoch [4/5], Step [900/2484], Loss: 2.7044\n",
      "Epoch [4/5], Step [1000/2484], Loss: 2.5845\n",
      "Epoch [4/5], Step [1100/2484], Loss: 2.7338\n",
      "Epoch [4/5], Step [1200/2484], Loss: 2.7184\n",
      "Epoch [4/5], Step [1300/2484], Loss: 2.5929\n",
      "Epoch [4/5], Step [1400/2484], Loss: 2.6783\n",
      "Epoch [4/5], Step [1500/2484], Loss: 2.6417\n",
      "Epoch [4/5], Step [1600/2484], Loss: 2.7864\n",
      "Epoch [4/5], Step [1700/2484], Loss: 2.6498\n",
      "Epoch [4/5], Step [1800/2484], Loss: 2.6027\n",
      "Epoch [4/5], Step [1900/2484], Loss: 2.8025\n",
      "Epoch [4/5], Step [2000/2484], Loss: 2.5444\n",
      "Epoch [4/5], Step [2100/2484], Loss: 2.6499\n",
      "Epoch [4/5], Step [2200/2484], Loss: 2.7351\n",
      "Epoch [4/5], Step [2300/2484], Loss: 2.7472\n",
      "Epoch [4/5], Step [2400/2484], Loss: 2.7170\n",
      "Epoch [4/5], Avg Loss: 2.6596\n",
      "Epoch [5/5], Step [0/2484], Loss: 2.6456\n",
      "Epoch [5/5], Step [100/2484], Loss: 2.6802\n",
      "Epoch [5/5], Step [200/2484], Loss: 2.6352\n",
      "Epoch [5/5], Step [300/2484], Loss: 2.5534\n",
      "Epoch [5/5], Step [400/2484], Loss: 2.4846\n",
      "Epoch [5/5], Step [500/2484], Loss: 2.6533\n",
      "Epoch [5/5], Step [600/2484], Loss: 2.7188\n",
      "Epoch [5/5], Step [700/2484], Loss: 2.4884\n",
      "Epoch [5/5], Step [800/2484], Loss: 2.6703\n",
      "Epoch [5/5], Step [900/2484], Loss: 2.6413\n",
      "Epoch [5/5], Step [1000/2484], Loss: 2.5931\n",
      "Epoch [5/5], Step [1100/2484], Loss: 2.5287\n",
      "Epoch [5/5], Step [1200/2484], Loss: 2.4300\n",
      "Epoch [5/5], Step [1300/2484], Loss: 2.7172\n",
      "Epoch [5/5], Step [1400/2484], Loss: 2.5981\n",
      "Epoch [5/5], Step [1500/2484], Loss: 2.5605\n",
      "Epoch [5/5], Step [1600/2484], Loss: 2.6127\n",
      "Epoch [5/5], Step [1700/2484], Loss: 2.4625\n",
      "Epoch [5/5], Step [1800/2484], Loss: 2.7106\n",
      "Epoch [5/5], Step [1900/2484], Loss: 2.5349\n",
      "Epoch [5/5], Step [2000/2484], Loss: 2.6072\n",
      "Epoch [5/5], Step [2100/2484], Loss: 2.4763\n",
      "Epoch [5/5], Step [2200/2484], Loss: 2.7396\n",
      "Epoch [5/5], Step [2300/2484], Loss: 2.6208\n",
      "Epoch [5/5], Step [2400/2484], Loss: 2.4902\n",
      "Epoch [5/5], Avg Loss: 2.5934\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (images, captions) in enumerate(loader):\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "        # Feed everything except the last token as input\n",
    "        inputs = captions[:, :-1]      # <start> ... <word_n-1>\n",
    "        targets = captions[:, 1:]      # <word_1> ... <end>\n",
    "\n",
    "        outputs = model(images, inputs)  # (batch, seq_len-1, vocab_size)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, vocab_size),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"loss\": avg_loss\n",
    "    }\n",
    "    torch.save(checkpoint, f\"caption_model_epoch{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "986a75e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"caption_model_final.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c97ce80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a6387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7391/3571093391.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"caption_model_epoch5.pth\", map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "checkpoint = torch.load(\"caption_model_epoch5.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b705a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with fresh optimizer state (better for LR change)\n",
      "FIXED training from epoch 5\n",
      "New learning rate: 2e-5 (much lower!)\n",
      "New batch size: 32 (more stable)\n",
      "Target to beat: 2.5934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7391/2103278086.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "/home/sushi/Desktop/Projects/Image_captioning/ven/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Step [0/4967], Loss: 2.7071, LR: 0.0000200\n",
      "Epoch [6], Step [100/4967], Loss: 2.4724, LR: 0.0000200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     84\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è No improvement (current: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m vs best: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Stop current training and restart with fixes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[43mfixed_resume_training\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcaption_model_epoch5.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_additional_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mfixed_resume_training\u001b[39m\u001b[34m(checkpoint_path, model, vocab, dataset, num_additional_epochs)\u001b[39m\n\u001b[32m     55\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)  \u001b[38;5;66;03m# Lower clipping\u001b[39;00m\n\u001b[32m     56\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     61\u001b[39m     current_lr = optimizer.param_groups[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def fixed_resume_training(checkpoint_path, model, vocab, dataset, num_additional_epochs=10):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4, betas=(0.9, 0.999))\n",
    "    \n",
    "    print(\"Starting with fresh optimizer state (better for LR change)\")\n",
    "    \n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, \n",
    "                       num_workers=4, pin_memory=True)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\n",
    "    \n",
    "    start_epoch = checkpoint[\"epoch\"]\n",
    "    best_loss = checkpoint[\"loss\"]\n",
    "    \n",
    "    print(f\"FIXED training from epoch {start_epoch}\")\n",
    "    print(f\"New learning rate: 2e-5 (much lower!)\")\n",
    "    print(f\"New batch size: 32 (more stable)\")\n",
    "    print(f\"Target to beat: {best_loss:.4f}\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, start_epoch + num_additional_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = len(loader)\n",
    "        \n",
    "        for i, (images, captions) in enumerate(loader):\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            \n",
    "            inputs = captions[:, :-1]\n",
    "            targets = captions[:, 1:]\n",
    "            \n",
    "            outputs = model(images, inputs)\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, len(vocab)),\n",
    "                targets.reshape(-1)\n",
    "            )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Lower clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch [{epoch+1}], Step [{i}/{num_batches}], \"\n",
    "                      f\"Loss: {loss.item():.4f}, LR: {current_lr:.7f}\")\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch [{epoch+1}], Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            improvement = ((checkpoint[\"loss\"] - avg_loss) / checkpoint[\"loss\"]) * 100\n",
    "            torch.save({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"scheduler_state\": scheduler.state_dict(),\n",
    "                \"loss\": avg_loss\n",
    "            }, f\"caption_model_epoch{epoch+1}_fixed.pth\")\n",
    "            print(f\"‚úÖ IMPROVEMENT! Saved model with loss: {avg_loss:.4f} ({improvement:.1f}% better)\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No improvement (current: {avg_loss:.4f} vs best: {best_loss:.4f})\")\n",
    "\n",
    "fixed_resume_training(\"caption_model_epoch5.pth\", model, vocab, dataset, num_additional_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d5cb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 128714\n",
      "Validation samples: 14305\n",
      "Test samples: 15895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "unique_images = df['image'].unique()\n",
    "\n",
    "train_val_images, test_images = train_test_split(unique_images, test_size=0.1, random_state=42)\n",
    "\n",
    "train_images, val_images = train_test_split(train_val_images, test_size=0.1, random_state=42)\n",
    "train_df = df[df['image'].isin(train_images)].reset_index(drop=True)\n",
    "val_df = df[df['image'].isin(val_images)].reset_index(drop=True)\n",
    "test_df = df[df['image'].isin(test_images)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5cf69c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FlickrDataset(train_df, image_dir, vocab, transform)\n",
    "val_dataset = FlickrDataset(val_df, image_dir, vocab, transform)\n",
    "test_dataset = FlickrDataset(test_df, image_dir, vocab, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageCaptioningModel(embed_size=256, hidden_size=512, vocab_size=len(vocab)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9476a048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7391/1330455400.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"caption_model_epoch15_fixed.pth\", map_location='cpu')  # or whatever your checkpoint is named\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"caption_model_epoch15_fixed.pth\", map_location='cpu')  # or whatever your checkpoint is named\n",
    "model.load_state_dict(checkpoint[\"model_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2002e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded existing optimizer state\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=5e-5) \n",
    "try:\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    print(\"‚úÖ Loaded existing optimizer state\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Starting with fresh optimizer state\")\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.7)  # Reduce LR by 30% every 3 epochs\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076749a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 16\n",
      "Will train until epoch 30\n"
     ]
    }
   ],
   "source": [
    "start_epoch = checkpoint.get(\"epoch\", 15)  \n",
    "additional_epochs = 14 \n",
    "total_epochs = start_epoch + additional_epochs\n",
    "\n",
    "print(f\"Resuming training from epoch {start_epoch}\")\n",
    "print(f\"Will train until epoch {total_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd80c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0 \n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"‚ö†Ô∏è EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"üõë Early stopping triggered!\")\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d14ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30], Step [0/2012], Loss: 2.5145, LR: 0.000020\n",
      "Epoch [17/30], Step [200/2012], Loss: 2.5323, LR: 0.000020\n",
      "Epoch [17/30], Step [400/2012], Loss: 2.4555, LR: 0.000020\n",
      "Epoch [17/30], Step [600/2012], Loss: 2.5598, LR: 0.000020\n",
      "Epoch [17/30], Step [800/2012], Loss: 2.5073, LR: 0.000020\n",
      "Epoch [17/30], Step [1000/2012], Loss: 2.5879, LR: 0.000020\n",
      "Epoch [17/30], Step [1200/2012], Loss: 2.4503, LR: 0.000020\n",
      "Epoch [17/30], Step [1400/2012], Loss: 2.4339, LR: 0.000020\n",
      "Epoch [17/30], Step [1600/2012], Loss: 2.4911, LR: 0.000020\n",
      "Epoch [17/30], Step [1800/2012], Loss: 2.4284, LR: 0.000020\n",
      "Epoch [17/30], Step [2000/2012], Loss: 2.6117, LR: 0.000020\n",
      "Epoch [17/30]\n",
      "  Train Loss: 2.4805\n",
      "  Val Loss: 2.4493\n",
      "  Learning Rate: 0.000020\n",
      "  üíæ Saved checkpoint\n",
      "--------------------------------------------------\n",
      "Epoch [18/30], Step [0/2012], Loss: 2.5959, LR: 0.000020\n",
      "Epoch [18/30], Step [200/2012], Loss: 2.4939, LR: 0.000020\n",
      "Epoch [18/30], Step [400/2012], Loss: 2.3293, LR: 0.000020\n",
      "Epoch [18/30], Step [600/2012], Loss: 2.4475, LR: 0.000020\n",
      "Epoch [18/30], Step [800/2012], Loss: 2.3796, LR: 0.000020\n",
      "Epoch [18/30], Step [1000/2012], Loss: 2.4612, LR: 0.000020\n",
      "Epoch [18/30], Step [1200/2012], Loss: 2.4200, LR: 0.000020\n",
      "Epoch [18/30], Step [1400/2012], Loss: 2.4831, LR: 0.000020\n",
      "Epoch [18/30], Step [1600/2012], Loss: 2.5793, LR: 0.000020\n",
      "Epoch [18/30], Step [1800/2012], Loss: 2.5266, LR: 0.000020\n",
      "Epoch [18/30], Step [2000/2012], Loss: 2.4017, LR: 0.000020\n",
      "Epoch [18/30]\n",
      "  Train Loss: 2.4890\n",
      "  Val Loss: 2.4634\n",
      "  Learning Rate: 0.000020\n",
      "‚ö†Ô∏è EarlyStopping counter: 1 out of 3\n",
      "  üíæ Saved checkpoint\n",
      "--------------------------------------------------\n",
      "Epoch [19/30], Step [0/2012], Loss: 2.4537, LR: 0.000020\n",
      "Epoch [19/30], Step [200/2012], Loss: 2.4768, LR: 0.000020\n",
      "Epoch [19/30], Step [400/2012], Loss: 2.2924, LR: 0.000020\n",
      "Epoch [19/30], Step [600/2012], Loss: 2.8450, LR: 0.000020\n",
      "Epoch [19/30], Step [800/2012], Loss: 2.4227, LR: 0.000020\n",
      "Epoch [19/30], Step [1000/2012], Loss: 2.5832, LR: 0.000020\n",
      "Epoch [19/30], Step [1200/2012], Loss: 2.4841, LR: 0.000020\n",
      "Epoch [19/30], Step [1400/2012], Loss: 2.4308, LR: 0.000020\n",
      "Epoch [19/30], Step [1600/2012], Loss: 2.4742, LR: 0.000020\n",
      "Epoch [19/30], Step [1800/2012], Loss: 2.5345, LR: 0.000020\n",
      "Epoch [19/30], Step [2000/2012], Loss: 2.5326, LR: 0.000020\n",
      "Epoch [19/30]\n",
      "  Train Loss: 2.4963\n",
      "  Val Loss: 2.4774\n",
      "  Learning Rate: 0.000014\n",
      "‚ö†Ô∏è EarlyStopping counter: 2 out of 3\n",
      "  üíæ Saved checkpoint\n",
      "--------------------------------------------------\n",
      "Epoch [20/30], Step [0/2012], Loss: 2.5961, LR: 0.000014\n",
      "Epoch [20/30], Step [200/2012], Loss: 2.4820, LR: 0.000014\n",
      "Epoch [20/30], Step [400/2012], Loss: 2.3733, LR: 0.000014\n",
      "Epoch [20/30], Step [600/2012], Loss: 2.5462, LR: 0.000014\n",
      "Epoch [20/30], Step [800/2012], Loss: 2.5805, LR: 0.000014\n",
      "Epoch [20/30], Step [1000/2012], Loss: 2.2426, LR: 0.000014\n",
      "Epoch [20/30], Step [1200/2012], Loss: 2.6131, LR: 0.000014\n",
      "Epoch [20/30], Step [1400/2012], Loss: 2.3783, LR: 0.000014\n",
      "Epoch [20/30], Step [1600/2012], Loss: 2.5242, LR: 0.000014\n",
      "Epoch [20/30], Step [1800/2012], Loss: 2.5582, LR: 0.000014\n",
      "Epoch [20/30], Step [2000/2012], Loss: 2.5531, LR: 0.000014\n",
      "Epoch [20/30]\n",
      "  Train Loss: 2.5009\n",
      "  Val Loss: 2.4853\n",
      "  Learning Rate: 0.000014\n",
      "‚ö†Ô∏è EarlyStopping counter: 3 out of 3\n",
      "üõë Early stopping triggered!\n",
      "üõë Training stopped early to prevent overfitting!\n",
      "‚úÖ Training completed!\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01)\n",
    "\n",
    "for epoch in range(start_epoch, total_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for batch_idx, (images, captions) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        inputs = captions[:, :-1]\n",
    "        targets = captions[:, 1:]\n",
    "        \n",
    "        outputs = model(images, inputs)\n",
    "        loss = criterion(outputs.reshape(-1, len(vocab)), targets.reshape(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{total_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, captions in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            inputs = captions[:, :-1]\n",
    "            targets = captions[:, 1:]\n",
    "            \n",
    "            outputs = model(images, inputs)\n",
    "            loss = criterion(outputs.reshape(-1, len(vocab)), targets.reshape(-1))\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{total_epochs}]\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    early_stopping(avg_val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"üõë Training stopped early to prevent overfitting!\")\n",
    "        break\n",
    "    \n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"val_loss\": avg_val_loss,\n",
    "    }\n",
    "    torch.save(checkpoint, f\"continued_model_epoch{epoch+1}.pth\")\n",
    "    print(f\"  üíæ Saved checkpoint\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"‚úÖ Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ab08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TESTING FINAL MODEL QUALITY\n",
      "==================================================\n",
      "üì∑ Image: 1000092795.jpg\n",
      "ü§ñ Generated Caption: A man in a blue shirt is a a in . of.\n",
      "------------------------------\n",
      "üì∑ Image: 10002456.jpg\n",
      "ü§ñ Generated Caption: Two men are working on a.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_images = [\"1000092795.jpg\", \"10002456.jpg\"]\n",
    "\n",
    "print(\"üß™ TESTING FINAL MODEL QUALITY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for img_name in test_images:\n",
    "    img_path = f\"flickr30k_images/{img_name}\"\n",
    "    \n",
    "    caption = generate_caption_beam_search(img_path, model, vocab, device, beam_width=5)\n",
    "    \n",
    "    print(f\"üì∑ Image: {img_name}\")\n",
    "    print(f\"ü§ñ Generated Caption: {caption}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac7c4ad",
   "metadata": {},
   "source": [
    "## üìä Model Evaluation\n",
    "\n",
    "Testing the trained model and computing evaluation metrics like BLEU scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da509260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_caption(caption_words):\n",
    "    import re\n",
    "    \n",
    "    if isinstance(caption_words, list):\n",
    "        caption = \" \".join(caption_words)\n",
    "    else:\n",
    "        caption = str(caption_words)\n",
    "    \n",
    "    caption = re.sub(r'\\b(start|end)\\b', '', caption, flags=re.IGNORECASE)\n",
    "    \n",
    "    caption = re.sub(r'[<>]', '', caption)  # Remove < >\n",
    "    caption = re.sub(r'\\s+', ' ', caption)  # Multiple spaces to single\n",
    "    caption = re.sub(r'^\\W+|\\W+$', '', caption)  # Leading/trailing punctuation\n",
    "    \n",
    "    caption = caption.strip()\n",
    "    if caption:\n",
    "        caption = caption[0].upper() + caption[1:] if len(caption) > 1 else caption.upper()\n",
    "        if not caption.endswith('.'):\n",
    "            caption += '.'\n",
    "    \n",
    "    return caption if caption else \"No caption generated.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_better_caption(image_path, model, vocab, device, max_len=15, beam_width=5):\n",
    "    from PIL import Image\n",
    "    import torch\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(image)\n",
    "        \n",
    "        sequences = [([vocab.stoi[\"<start>\"]], 0.0)]\n",
    "        \n",
    "        for step in range(max_len):\n",
    "            all_candidates = []\n",
    "            \n",
    "            for seq, score in sequences:\n",
    "                if seq[-1] == vocab.stoi[\"<end>\"]:\n",
    "                    all_candidates.append((seq, score))\n",
    "                    continue\n",
    "                \n",
    "                input_seq = torch.tensor([seq]).to(device)\n",
    "                \n",
    "                outputs = model.decoder(features, input_seq)\n",
    "                probs = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "                \n",
    "                top_probs, top_indices = torch.topk(probs, beam_width)\n",
    "                \n",
    "                for prob, idx in zip(top_probs[0], top_indices[0]):\n",
    "                    candidate_seq = seq + [idx.item()]\n",
    "                    candidate_score = score - torch.log(prob).item()\n",
    "                    all_candidates.append((candidate_seq, candidate_score))\n",
    "            \n",
    "            sequences = sorted(all_candidates, key=lambda x: x[1])[:beam_width]\n",
    "            \n",
    "            if all(seq[0][-1] == vocab.stoi[\"<end>\"] for seq in sequences):\n",
    "                break\n",
    "        \n",
    "        best_sequence = sequences[0][0]\n",
    "        \n",
    "        words = [vocab.itos[idx] for idx in best_sequence \n",
    "                if idx in vocab.itos and vocab.itos[idx] not in [\"<start>\", \"<end>\", \"<pad>\"]]\n",
    "        \n",
    "        # Join and clean\n",
    "        caption = \" \".join(words)\n",
    "        return sanitize_caption(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4248734d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß TESTING IMPROVED CAPTION GENERATION\n",
      "==================================================\n",
      "\n",
      "üì∑ 2878190821.jpg\n",
      "‚ùå Old: A little boy in a blue shirt is a a in.\n",
      "‚úÖ New: A little boy in a blue shirt and a shorts hat a.\n",
      "----------------------------------------\n",
      "\n",
      "üì∑ 617999370.jpg\n",
      "‚ùå Old: A man in a blue shirt is a a in in of of.\n",
      "‚úÖ New: A man in a blue shirt is a a in on of.\n",
      "----------------------------------------\n",
      "\n",
      "üì∑ 4557307607.jpg\n",
      "‚ùå Old: A group of people are sitting around a table.\n",
      "‚úÖ New: A group of people are sitting around a table . a.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß TESTING IMPROVED CAPTION GENERATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_images = [\"2878190821.jpg\", \"617999370.jpg\", \"4557307607.jpg\"]\n",
    "\n",
    "for img_name in test_images:\n",
    "    img_path = f\"flickr30k_images/{img_name}\"\n",
    "    \n",
    "    # Original method\n",
    "    old_caption = generate_caption_beam_search(img_path, model, vocab, device, beam_width=5)\n",
    "    \n",
    "    # Improved method\n",
    "    new_caption = generate_better_caption(img_path, model, vocab, device, beam_width=5)\n",
    "    \n",
    "    print(f\"\\nüì∑ {img_name}\")\n",
    "    print(f\"‚ùå Old: {old_caption}\")\n",
    "    print(f\"‚úÖ New: {new_caption}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc34186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ <start>: index 1\n",
      "‚úÖ <end>: index 2\n",
      "‚úÖ <pad>: index 0\n",
      "‚úÖ <unk>: index 3\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['<start>', '<end>', '<pad>', '<unk>']\n",
    "for token in special_tokens:\n",
    "    if token in vocab.stoi:\n",
    "        print(f\"‚úÖ {token}: index {vocab.stoi[token]}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {token}: missing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94006365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_improved_vocab(df, min_freq=3, max_vocab=10000):\n",
    "    from collections import Counter\n",
    "    import re\n",
    "    \n",
    "    all_words = []\n",
    "    for caption in df['caption']:\n",
    "        # Clean caption\n",
    "        clean_caption = re.sub(r'[^\\w\\s]', ' ', caption.lower())\n",
    "        words = clean_caption.split()\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    # Count frequencies\n",
    "    word_freq = Counter(all_words)\n",
    "    \n",
    "    # Create vocabulary\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "    \n",
    "    common_words = word_freq.most_common(max_vocab - 4)\n",
    "    for word, freq in common_words:\n",
    "        if freq >= min_freq and len(word) > 1:  # Skip single characters\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b209f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f7da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_repetitive_caption(caption):\n",
    "    \"\"\"Remove repetitive words and fix common patterns\"\"\"\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Split into words\n",
    "    words = caption.strip().split()\n",
    "    if not words:\n",
    "        return \"No caption available.\"\n",
    "    \n",
    "    clean_words = [words[0]]\n",
    "    for word in words[1:]:\n",
    "        if word != clean_words[-1]:\n",
    "            clean_words.append(word)\n",
    "    \n",
    "    final_words = []\n",
    "    for i, word in enumerate(clean_words):\n",
    "        recent_context = final_words[-3:] if len(final_words) >= 3 else final_words\n",
    "        if recent_context.count(word) < 2:  # Allow max 2 occurrences in recent context\n",
    "            final_words.append(word)\n",
    "    \n",
    "    text = \" \".join(final_words)\n",
    "    \n",
    "    # Pattern fixes\n",
    "    replacements = [\n",
    "        (r'\\ba a\\b', 'a'),\n",
    "        (r'\\bin in\\b', 'in'),  \n",
    "        (r'\\bof of\\b', 'of'),\n",
    "        (r'\\bis a in\\b', 'is standing in'),\n",
    "        (r'\\bis a a\\b', 'is wearing a'),\n",
    "        (r'\\bare a on\\b', 'are sitting on'),\n",
    "        (r'\\band a shorts hat a\\b', 'wearing shorts and a hat'),\n",
    "        (r'\\s+\\.\\s*a\\s*$', '.'),  # Remove trailing \". a\"\n",
    "        (r'\\s+a\\s*$', '.'),       # Remove trailing \"a\"\n",
    "    ]\n",
    "    \n",
    "    for pattern, replacement in replacements:\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    if text:\n",
    "        text = text[0].upper() + text[1:] if len(text) > 1 else text.upper()\n",
    "        if not text.endswith('.'):\n",
    "            text += '.'\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260ca664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_with_penalty(model, image_tensor, vocab, device, beam_width=5, max_len=15, repetition_penalty=1.2):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model.encoder(image_tensor)\n",
    "        \n",
    "        sequences = [([vocab.stoi[\"<start>\"]], 0.0)]\n",
    "        \n",
    "        for step in range(max_len):\n",
    "            all_candidates = []\n",
    "            \n",
    "            for seq, score in sequences:\n",
    "                if seq[-1] == vocab.stoi[\"<end>\"]:\n",
    "                    all_candidates.append((seq, score))\n",
    "                    continue\n",
    "                \n",
    "                input_seq = torch.tensor([seq]).to(device)\n",
    "                outputs = model.decoder(features, input_seq)\n",
    "                log_probs = torch.log_softmax(outputs[:, -1, :], dim=-1)\n",
    "                \n",
    "                for word_idx in set(seq[1:]): \n",
    "                    if word_idx in vocab.itos:\n",
    "                        log_probs[0, word_idx] /= repetition_penalty\n",
    "                \n",
    "                top_log_probs, top_indices = torch.topk(log_probs, beam_width)\n",
    "                \n",
    "                for log_prob, idx in zip(top_log_probs[0], top_indices[0]):\n",
    "                    candidate_seq = seq + [idx.item()]\n",
    "                    candidate_score = score - log_prob.item()\n",
    "                    all_candidates.append((candidate_seq, candidate_score))\n",
    "            \n",
    "            sequences = sorted(all_candidates, key=lambda x: x[1])[:beam_width]\n",
    "            \n",
    "            if all(seq[0][-1] == vocab.stoi[\"<end>\"] for seq in sequences):\n",
    "                break\n",
    "        \n",
    "        best_sequence = sequences[0][0]\n",
    "        \n",
    "        words = []\n",
    "        for idx in best_sequence[1:-1]:\n",
    "            if idx in vocab.itos:\n",
    "                words.append(vocab.itos[idx])\n",
    "        \n",
    "        caption = \" \".join(words)\n",
    "        return fix_repetitive_caption(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_caption(image_path, model, vocab, device):\n",
    "    from PIL import Image\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    caption = beam_search_with_penalty(\n",
    "        model, image, vocab, device, \n",
    "        beam_width=5, max_len=15, repetition_penalty=1.3\n",
    "    )\n",
    "    \n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b87c6",
   "metadata": {},
   "source": [
    "### Sample Results\n",
    "\n",
    "Visualizing some example images with their generated captions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba784c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß TESTING FINAL IMPROVED CAPTION GENERATION\n",
      "============================================================\n",
      "\n",
      "üì∑ 2878190821.jpg\n",
      "‚ùå Current: A little boy in a blue shirt is a a in.\n",
      "‚úÖ Fixed: < start > a young boy in a blue shirt is standing in.\n",
      "--------------------------------------------------\n",
      "\n",
      "üì∑ 617999370.jpg\n",
      "‚ùå Current: A man in a blue shirt is a a in in of of.\n",
      "‚úÖ Fixed: < start > a man in a blue shirt is standing in of.\n",
      "--------------------------------------------------\n",
      "\n",
      "üì∑ 4557307607.jpg\n",
      "‚ùå Current: A group of people are sitting around a table.\n",
      "‚úÖ Fixed: < start > a group of people sitting a . end.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß TESTING FINAL IMPROVED CAPTION GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_images = [\"2878190821.jpg\", \"617999370.jpg\", \"4557307607.jpg\"]\n",
    "\n",
    "for img_name in test_images:\n",
    "    img_path = f\"flickr30k_images/{img_name}\"\n",
    "    \n",
    "    # current method\n",
    "    old_caption = generate_caption_beam_search(img_path, model, vocab, device, beam_width=5)\n",
    "    \n",
    "    # New improved method\n",
    "    final_caption = generate_final_caption(img_path, model, vocab, device)\n",
    "    \n",
    "    print(f\"\\nüì∑ {img_name}\")\n",
    "    print(f\"‚ùå Current: {old_caption}\")\n",
    "    print(f\"‚úÖ Fixed: {final_caption}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ceecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
